{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1-Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "\n",
    "# Import the required libraries\n",
    "import configparser\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hadoop_config_file.xml')\n",
    "\n",
    "# Extract the core component details\n",
    "core_components = config['core']\n",
    "print('Core Components:')\n",
    "for component in core_components:\n",
    "    print(component)\n",
    "Q2-Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "\n",
    "# Import the required libraries\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_directory_size(hdfs_url, directory_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Calculate the total file size in the directory\n",
    "    total_size = 0\n",
    "    files = client.list(directory_path, status=True)\n",
    "    for file in files:\n",
    "        if file['type'] == 'FILE':\n",
    "            total_size += file['length']\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "# Usage example\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "directory_path = '/user/data'\n",
    "total_size = calculate_directory_size(hdfs_url, directory_path)\n",
    "print(f'Total file size in {directory_path}: {total_size} bytes')\n",
    "\n",
    "Q3-Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "\n",
    "# Import the required libraries\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--n', type=int, help='Number of top words to retrieve')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_top_n_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = re.findall(r'\\w+', line)\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_top_n_words(self, _, word_counts):\n",
    "        n = self.options.n\n",
    "        sorted_word_counts = sorted(word_counts, reverse=True)\n",
    "        for i in range(n):\n",
    "            yield sorted_word_counts[i][1], sorted_word_counts[i][0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n",
    "Q4-Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "python\n",
    "Copy code\n",
    "# Import the required libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# NameNode health check\n",
    "namenode_url = 'http://localhost:50070'\n",
    "namenode_health_url = f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'\n",
    "namenode_response = requests.get(namenode_health_url)\n",
    "namenode_data = json.loads(namenode_response.content)\n",
    "namenode_health_status = namenode_data['beans'][0]['State']\n",
    "\n",
    "print(f'NameNode Health Status: {namenode_health_status}')\n",
    "\n",
    "# DataNode health check\n",
    "datanode_url = 'http://localhost:50075'\n",
    "datanode_health_url = f'{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo'\n",
    "datanode_response = requests.get(datanode_health_url)\n",
    "datanode_data = json.loads(datanode_response.content)\n",
    "datanode_health_status = datanode_data['beans'][0]['State']\n",
    "\n",
    "print(f'DataNode Health Status: {datanode_health_status}')\n",
    "Q5-Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "\n",
    "# Import the required libraries\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, directory_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # List all files and directories in the HDFS path\n",
    "    paths = client.list(directory_path)\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "\n",
    "# Usage example\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "directory_path = '/user/data'\n",
    "list_hdfs_path(hdfs_url, directory_path)\n",
    "Q6-Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "\n",
    "# Import the required libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# DataNode storage utilization analysis\n",
    "datanode_url = 'http://localhost:50075'\n",
    "datanode_info_url = f'{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo'\n",
    "datanode_response = requests.get(datanode_info_url)\n",
    "datanode_data = json.loads(datanode_response.content)\n",
    "datanode_info = datanode_data['beans'][0]\n",
    "storage_report = datanode_info['StorageInfo']['storageReport']\n",
    "\n",
    "# Find the node with the highest storage capacity\n",
    "max_storage_node = max(storage_report, key=lambda x: x['capacity'])\n",
    "max_storage_capacity = max_storage_node['capacity']\n",
    "\n",
    "# Find the node with the lowest storage capacity\n",
    "min_storage_node = min(storage_report, key=lambda x: x['capacity'])\n",
    "min_storage_capacity = min_storage_node['capacity']\n",
    "\n",
    "print(f'Highest Storage Capacity: {max_storage_capacity}')\n",
    "print(f'Lowest Storage Capacity: {min_storage_capacity}')\n",
    "Q7-Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "\n",
    "# Import the required libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Submit a Hadoop job to YARN\n",
    "resource_manager_url = 'http://localhost:8088'\n",
    "submit_job_url = f'{resource_manager_url}/ws/v1/cluster/apps/new-application'\n",
    "job_submission_response = requests.post(submit_job_url)\n",
    "job_submission_data = json.loads(job_submission_response.content)\n",
    "application_id = job_submission_data['application-id']\n",
    "\n",
    "# Monitor job progress\n",
    "application_url = f'{resource_manager_url}/ws/v1/cluster/apps/{application_id}'\n",
    "application_response = requests.get(application_url)\n",
    "application_data = json.loads(application_response.content)\n",
    "progress = application_data['app']['progress']\n",
    "\n",
    "# Retrieve job output\n",
    "output_url = f'{resource_manager_url}/ws/v1/cluster/apps/{application_id}/state'\n",
    "output_response = requests.get(output_url)\n",
    "output_data = json.loads(output_response.content)\n",
    "final_output = output_data['app']['finalOutput']\n",
    "\n",
    "print(f'Job Progress: {progress}')\n",
    "print(f'Job Final Output: {final_output}')\n",
    "Q8-Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "\n",
    "# Import the required libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Submit a Hadoop job to YARN with resource requirements\n",
    "resource_manager_url = 'http://localhost:8088'\n",
    "submit_job_url = f'{resource_manager_url}/ws/v1/cluster/apps/new-application'\n",
    "job_submission_response = requests.post(submit_job_url)\n",
    "job_submission_data = json.loads(job_submission_response.content)\n",
    "application_id = job_submission_data['application-id']\n",
    "\n",
    "# Set resource requirements for the job\n",
    "resource_requirements = {\n",
    "    'memory': 2048,\n",
    "    'vCores': 2\n",
    "}\n",
    "resource_requirements_url = f'{resource_manager_url}/ws/v1/cluster/apps/{application_id}'\n",
    "resource_requirements_response = requests.put(resource_requirements_url, json=resource_requirements)\n",
    "\n",
    "# Track resource usage during job execution\n",
    "resource_usage_url = f'{resource_manager_url}/ws/v1/cluster/apps/{application_id}/resource-usage'\n",
    "resource_usage_response = requests.get(resource_usage_url)\n",
    "resource_usage_data = json.loads(resource_usage_response.content)\n",
    "current_memory_usage = resource_usage_data['app']['resourceUsage']['memorySeconds']\n",
    "current_vcores_usage = resource_usage_data['app']['resourceUsage']['vcoreSeconds']\n",
    "\n",
    "print(f'Current Memory Usage: {current_memory_usage}')\n",
    "print(f'Current vCores Usage: {current_vcores_usage}')\n",
    "\n",
    "Q9-Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "\n",
    "# Import the required libraries\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.conf import combine_dicts\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class PerformanceComparison(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(PerformanceComparison, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, help='Input split size in MB')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Perform some mapping operation\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # Perform some reducing operation\n",
    "        time.sleep(0.1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PerformanceComparison.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
